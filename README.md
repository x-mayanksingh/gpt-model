# gpt-model
Made a gpt model using extensive machine learning.This repo holds the from-scratch reproduction of nanoGPT, The finetuning process (while quite simple conceptually - SFT is just about swapping out the dataset and continuing the training) comes after this part and will be covered at a later time. For now this is the kind of stuff that the 124M model says if you prompt it with "Hello, I'm a language model," after 10B tokens of training:

Hello, I'm a language model, and my goal is to make English as easy and fun as possible for everyone, and to find out the different grammar rules<br />
Hello, I'm a language model, so the next time I go, I'll just say, I like this stuff.<br />
Hello, I'm a language model, and the question is, what should I do if I want to be a teacher?<br />
Hello, I'm a language model, and I'm an English person. In languages, "speak" is really speaking. Because for most people, there's<br />

And after 40B tokens of training:

Hello, I'm a language model, a model of computer science, and it's a way (in mathematics) to program computer programs to do things like write<br />
Hello, I'm a language model, not a human. This means that I believe in my language model, as I have no experience with it yet.<br />
Hello, I'm a language model, but I'm talking about data. You've got to create an array of data: you've got to create that.<br />
Hello, I'm a language model, and all of this is about modeling and learning Python. I'm very good in syntax, however I struggle with Python due<br />



